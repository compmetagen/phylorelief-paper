#! /usr/bin/env python

## This code is written by Davide Albanese, <davide.albanese@fmach.it>
## <davide.albanese@gmail.com>
## Copyright (C) 2014 Fondazione Edmund Mach
## Copyright (C) 2014 Davide Albanese

## This program is free software: you can redistribute it and/or modify
## it under the terms of the GNU General Public License as published by
## the Free Software Foundation, either version 3 of the License, or
## (at your option) any later version.

## This program is distributed in the hope that it will be useful,
## but WITHOUT ANY WARRANTY; without even the implied warranty of
## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
## GNU General Public License for more details.

## You should have received a copy of the GNU General Public License
## along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import division
import argparse
import csv
import os
import sys

import numpy as np
import scipy as sp
import pandas as pd
import dendropy

from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier

from phylorelief import prelief, import_data, KCCC_discrete


usage = """%(prog)s otu_table tree sample_data target [options]
"""

description = """PhyloRelief and Random Forest.
"""

epilog = """Example:

    $ ml_phylorelief otu_table.txt tree.tre sample_data.txt Status -k 1

Authors:
    Davide Albanese <davide.albanese@fmach.it>
    Claudio Donati <claudio.donati@fmach.it>

Fondazione Edmund Mach, 2014
"""

parser = argparse.ArgumentParser(
    usage=usage, description=description,
    formatter_class=argparse.RawDescriptionHelpFormatter,
    epilog=epilog)

parser.add_argument('otu_table',
                    help="an OTU table file (containing the number of "
                    "sequences observed in each OTU for each sample) in "
                    "tab-delimited format")
parser.add_argument('tree',
                    help="a tree in 'newick' format. Note that "
                    "all the leaf nodes should have univocal names")
parser.add_argument('sample_data',
                    help="a tab-delimited file containing sample information "
                    "and metadata")
parser.add_argument('target',
                    help="sample_data column to be used as class label")
parser.add_argument('-u', '--uf-variant', help="unifrac variant "
                    "(default %(default)s)",
                    choices=["unweighted", "weighted", "generalized"],
                    default="weighted")
parser.add_argument('-k', '--n-nearest-neighbors', help="number of nearest neighbors "
                    "(default %(default)s)", default=2, type=int)
parser.add_argument('-a', '--alpha', help="alpha for generalized unifrac "
                     "(default %(default)s)", default=0.5, type=float)
parser.add_argument('-i', '--n-iterations', help="number of iterations "
                     "(default %(default)s)", default=None, type=int)
parser.add_argument('-o', '--output-file', default="out.txt",
                    help="output file (default %(default)s)")
args = parser.parse_args()


def weight_otus(otu_table, weight, rank):
    w = pd.Series(index=otu_table.index)
    for node_oid, otus, _, _, _ in rank:
        for otu in otus:
            w[otu] = weight[node_oid]
    return w

# nested cross validation loop for feature selection2
def prelief_sel(otu_table, tree, target, classifier, k, m, variant, alpha):
    ns = range(1, 10, 1) + range(10, 100, 10) + range(100, 1000, 100)
    le = LabelEncoder()
    cv = StratifiedShuffleSplit(target.values, n_iter=10, test_size=0.25,
                                random_state=1)
    kccc = np.zeros((len(cv), len(ns)), dtype=np.float)
    for i, (train_index, test_index) in enumerate(cv):

        # train and test data
        otu_table_train = otu_table.iloc[:, train_index]
        otu_table_test = otu_table.iloc[:, test_index]
        target_train = target.iloc[train_index]
        target_test =  target.iloc[test_index]

        # phylorelief
        weight, rank = prelief(otu_table_train, tree, target_train, k=k, m=m,
                               variant=variant, alpha=alpha, equal_priors=True,
                               verbose=False)
        
        # weights each otu according the selected clade
        w = weight_otus(otu_table_train, weight, rank)

        # sort otus according their weights
        w.sort(ascending=False)

        # order the features according their weights and 
        # prepare for scikit-learn
        otu_table_train = otu_table_train.loc[w.index]
        otu_table_test = otu_table_test.loc[w.index]
        x_train = otu_table_train.T.values
        x_test = otu_table_test.T.values
        y_train = target_train.values
        y_test = target_test.values
        le.fit(y_train)
        y_train = le.transform(y_train)
        y_test = le.transform(y_test)

        # for each feature size learn the model and test
        for j, n in enumerate(ns):
            x_train_n, x_test_n = x_train[:, :n], x_test[:, :n]
            classifier.fit(x_train_n, y_train)
            y_true, y_pred = y_test, classifier.predict(x_test_n)
            kccc[i, j] = KCCC_discrete(y_true, y_pred)

    # find the maximum KCCC
    kccc_mean, kccc_sem = np.mean(kccc, axis=0), sp.stats.sem(kccc)
    idx_max = np.argmax(kccc_mean)
    n_zero = ns[idx_max]

    # phylorelief on the entire otu_table
    weight, rank = prelief(otu_table, tree, target, k=k, m=m, variant=variant,
                           alpha=alpha, equal_priors=True, verbose=False)

    # weights each otu according the selected clade
    w = weight_otus(otu_table, weight, rank)
    
    # sort otus according their weights
    w.sort(ascending=False)

    # n_zero selected OTUs
    selected = w.index.tolist()[:n_zero]
    
    return selected


otu_table, tree, sample_data = import_data(args.otu_table, args.tree, args.sample_data)

try:
    target = sample_data[args.target]
except KeyError:
    sys.stderr.write("target %s is not in the sample table")
    exit(1)

classifier = RandomForestClassifier(n_estimators=500, random_state=1)
cv = StratifiedShuffleSplit(target.values, n_iter=10, test_size=0.25,
                            random_state=1)
le = LabelEncoder()
kccc = np.empty(len(cv), dtype=np.float)
for i, (train_index, test_index) in enumerate(cv):

    sys.stdout.write('\r')
    sys.stdout.write("%d%%" % int(((i+1)/len(cv))*100))
    sys.stdout.flush()

    # train and test data
    otu_table_train = otu_table.iloc[:, train_index]
    otu_table_test = otu_table.iloc[:, test_index]
    target_train = target.iloc[train_index]
    target_test =  target.iloc[test_index]

    # nested cross validation loop for feature selection
    # on the training data
    selected = prelief_sel(otu_table_train, tree, target_train, classifier, 
                           k=args.n_nearest_neighbors, m=args.n_iterations,
                           variant=args.uf_variant, alpha=args.alpha)

    # filter OTU table with the selected OTUs
    otu_table_train = otu_table_train.loc[selected]
    otu_table_test = otu_table_test.loc[selected]

    # prepare for scikit-learn
    x_train = otu_table_train.T.values
    x_test = otu_table_test.T.values
    y_train = target_train.values
    y_test = target_test.values
    le.fit(y_train)
    y_train = le.transform(y_train)
    y_test = le.transform(y_test)

    # learn the model and test
    classifier.fit(x_train, y_train)
    y_true, y_pred = y_test, classifier.predict(x_test)
    kccc[i] = KCCC_discrete(y_true, y_pred)

kccc_mean, kccc_sem = sp.mean(kccc, axis=0), sp.stats.sem(kccc)

# perform the feature selection on the entire dataset
selected = prelief_sel(otu_table, tree, target, classifier, 
                       k=args.n_nearest_neighbors, m=args.n_iterations,
                       variant=args.uf_variant, alpha=args.alpha)

# write the output file
fout = open(args.output_file, 'w')
fout.write("%.3f\t%.3f\n" % (kccc_mean, kccc_sem))
fout.write('\t'.join(["%.3f" % elem for elem in kccc]) + '\n')
pd.Series(selected).to_csv(fout, index=False)
fout.close()
