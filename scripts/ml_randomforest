#! /usr/bin/env python

## This code is written by Davide Albanese, <davide.albanese@fmach.it>
## <davide.albanese@gmail.com>
## Copyright (C) 2014 Fondazione Edmund Mach
## Copyright (C) 2014 Davide Albanese

## This program is free software: you can redistribute it and/or modify
## it under the terms of the GNU General Public License as published by
## the Free Software Foundation, either version 3 of the License, or
## (at your option) any later version.

## This program is distributed in the hope that it will be useful,
## but WITHOUT ANY WARRANTY; without even the implied warranty of
## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
## GNU General Public License for more details.

## You should have received a copy of the GNU General Public License
## along with this program.  If not, see <http://www.gnu.org/licenses/>.


from __future__ import division
import argparse
import csv
import os
import sys

import numpy as np
import scipy as sp
import pandas as pd
import dendropy

from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier

from phylorelief import KCCC_discrete, import_otutable, import_sampledata


usage = """%(prog)s otu_table sample_data target
"""

description = """Random Forest.
"""

epilog = """Example:

    $ ml_randomforest otu_table.txt sample_data.txt Status

Authors:
    Davide Albanese <davide.albanese@fmach.it>
    Claudio Donati <claudio.donati@fmach.it>

Fondazione Edmund Mach, 2014
"""

parser = argparse.ArgumentParser(
    usage=usage, description=description,
    formatter_class=argparse.RawDescriptionHelpFormatter,
    epilog=epilog)

parser.add_argument('otu_table',
                    help="an OTU table file (containing the number of "
                    "sequences observed in each OTU for each sample) in "
                    "tab-delimited format")
parser.add_argument('sample_data',
                    help="a tab-delimited file containing sample information "
                    "and metadata")
parser.add_argument('target',
                    help="sample_data column to be used as class label")
parser.add_argument('-o', '--output-file', default="out.txt",
                    help="output file (default %(default)s)")
args = parser.parse_args()


def import_data(otutable_filename, sampledata_filename):
    otutable = import_otutable(otutable_filename)
    sampledata = import_sampledata(sampledata_filename)

    # otu table pruning
    otutable = otutable.loc[otutable.sum(axis=1) > 0]
    
    # align otutable and sampledata
    otutable = otutable.loc[:, sampledata.index]

    return otutable, sampledata

# nested cross validation loop for feature selection
def rf_sel(otu_table, target, classifier):
    ns = range(1, 10, 1) + range(10, 100, 10) + range(100, 1000, 100)
    le = LabelEncoder()
    cv = StratifiedShuffleSplit(target.values, n_iter=10, test_size=0.25,
                                random_state=1)
    kccc = np.zeros((len(cv), len(ns)), dtype=np.float)
    for i, (train_index, test_index) in enumerate(cv):

        # train and test data
        otu_table_train = otu_table.iloc[:, train_index]
        otu_table_test = otu_table.iloc[:, test_index]
        target_train = target.iloc[train_index]
        target_test =  target.iloc[test_index]

        # prepare for scikit-learn
        x_train = otu_table_train.T.values
        x_test = otu_table_test.T.values
        y_train = target_train.values
        y_test = target_test.values
        le.fit(y_train)
        y_train = le.transform(y_train)
        y_test = le.transform(y_test)

        # feature ranking
        classifier.fit(x_train, y_train)
        otu_rank = np.argsort(classifier.feature_importances_)[::-1]

        # order the features according their weights
        x_train = x_train[:, otu_rank]
        x_test = x_test[:, otu_rank]

        # for each feature size learn the model and test
        for j, n in enumerate(ns):
            x_train_n, x_test_n = x_train[:, :n], x_test[:, :n]
            classifier.fit(x_train_n, y_train)
            y_true, y_pred = y_test, classifier.predict(x_test_n)
            kccc[i, j] = KCCC_discrete(y_true, y_pred)

    # find the maximum KCCC
    kccc_mean, kccc_sem = np.mean(kccc, axis=0), sp.stats.sem(kccc)
    idx_max = np.argmax(kccc_mean)
    n_zero = ns[idx_max]

    # prepare for scikit-learn
    x = otu_table.T.values
    y = target.values
    le.fit(y)
    y = le.transform(y)

    # feature ranking and selection on the entire otu_table
    classifier.fit(x, y)
    rank = np.argsort(classifier.feature_importances_)[::-1]
    rank = rank[:n_zero]

    # n_zero selected OTUs
    selected = otu_table.index[rank].tolist()
    
    return selected


otu_table, sample_data = import_data(args.otu_table, args.sample_data)

try:
    target = sample_data[args.target]
except KeyError:
    sys.stderr.write("target %s is not in the sample table")
    exit(1)
    
classifier = RandomForestClassifier(n_estimators=500, random_state=1)
cv = StratifiedShuffleSplit(target.values, n_iter=10, test_size=0.25,
                            random_state=1)
le = LabelEncoder()
kccc = np.empty(len(cv), dtype=np.float)
for i, (train_index, test_index) in enumerate(cv):

    sys.stdout.write('\r')
    sys.stdout.write("%d%%" % int(((i+1)/len(cv))*100))
    sys.stdout.flush()

    # train and test data
    otu_table_train = otu_table.iloc[:, train_index]
    otu_table_test = otu_table.iloc[:, test_index]
    target_train = target.iloc[train_index]
    target_test =  target.iloc[test_index]

    # nested cross validation loop for feature selection
    # on the training data
    selected = rf_sel(otu_table_train, target_train, classifier)

    # filter OTU table with the selected OTUs
    otu_table_train = otu_table_train.loc[selected]
    otu_table_test = otu_table_test.loc[selected]

    # prepare for scikit-learn
    x_train = otu_table_train.T.values
    x_test = otu_table_test.T.values
    y_train = target_train.values
    y_test = target_test.values
    le.fit(y_train)
    y_train = le.transform(y_train)
    y_test = le.transform(y_test)

    # learn the model and test
    classifier.fit(x_train, y_train)
    y_true, y_pred = y_test, classifier.predict(x_test)
    kccc[i] = KCCC_discrete(y_true, y_pred)

kccc_mean, kccc_sem = sp.mean(kccc, axis=0), sp.stats.sem(kccc)

# perform the feature selection on the entire dataset
selected = rf_sel(otu_table_train, target_train, classifier)

# write the output file
fout = open(args.output_file, 'w')
fout.write("%.3f\t%.3f\n" % (kccc_mean, kccc_sem))
fout.write('\t'.join(["%.3f" % elem for elem in kccc]) + '\n')
pd.Series(selected).to_csv(fout, index=False)
fout.close()

